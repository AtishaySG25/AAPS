mean(simulatedData == 3)
mean(simulatedData <= 3)
r = 20
p = 0.2
j = 100
dnbinom(j-r, r, p)
j = 1000
dnbinom(j-r, r, p)
j = 20
dnbinom(j-r, r, p)
j = 50
dnbinom(j-r, r, p)
j = 100
dnbinom(j-r, r, p)
j = 200
dnbinom(j-r, r, p)
r = 20
p = 0.2
j = c(20:200)
dnbinom(j-r, r, p)
r = 20
p = 0.2
j = c(20:200)
cbind(j, dnbinom(j-r, r, p))
r = 20
p = 0.2
j = c(20:200)
as.data.frame(cbind(j, dnbinom(j-r, r, p)))
r = 20
p = 0.2
j = c(20:200)
df = as.data.frame(cbind(j, dnbinom(j-r, r, p)))
colnames(df) = c('j', 'probability')
head(df)
r = 20
p = 0.2
j = c(20:200)
df = as.data.frame(cbind(j, dnbinom(j-r, r, p)))
colnames(df) = c('j', 'probability')
p = ggplot(data = df) +
geom_bar(aes(x = j, y = probability), width = 0.1)
p
r = 20
p = 0.2
j = c(20:200)
df = as.data.frame(cbind(j, dnbinom(j-r, r, p)))
colnames(df) = c('j', 'probability')
p = ggplot(data = df) +
geom_col(aes(x = j, y = probability), width = 0.1)
p
r = 20
p = 0.2
j = c(20:200)
df = as.data.frame(cbind(j, dnbinom(j-r, r, p)))
colnames(df) = c('j', 'probability')
p = ggplot(data = df) +
geom_col(aes(x = j, y = probability))
p
x = c(1, -2, 10, 40, 15, -15, -20)
max(x)
argmax(x)
argmax?
x
argmax(x)
which.max(x)
x
max(x)
which.max(x)
library(ggplot2) # plotting library
library(dplyr)   # data wrangling library
# Generate a 52-card deck
suits = c('H', 'D', 'S', 'C')
cards = c(2:10, 'J', 'Q', 'K', 'A')
s = paste0(rep(cards, length(suits)), rep(suits, each = length(cards)))
# Simulate dealing 5 cards to 8 hands (40 cards)
nsimulations = 1e5
simulatedData = replicate(nsimulations, sample(s, size = 40, prob = rep(1/length(s), length(s))))
# Check event that exactly 3 aces are dealt
checkEvent1 = function(data){
return(sum(grepl('A', data)) == 3)
}
# Check event that 4 hands have exactly one ace each
checkEvent2 = function(data){
condition1 = (sum(grepl('A', data)) == 4)
condition2 = (length(unique(ceiling(which(grepl('A', data)) /5))) == 4)
return(condition1 & condition2)
}
# Check event that one hand has all four aces
checkEvent3 = function(data){
condition1 = (sum(grepl('A', data)) == 4)
condition2 = (length(unique(ceiling(which(grepl('A', data)) /5))) == 1)
return(condition1 & condition2)
}
# Check event given that the ace of spades was among the cards dealt, that it is in a hand with at least one other spade.
checkEvent4 = function(data){
hand = ceiling(which(data == 'AS') / 5)
return('AH' %in% data[c((1+(hand-1)*5):(hand*5))] | 'AD' %in% data[c((1+(hand-1)*5):(hand*5))] | 'AC' %in% data[c((1+(hand-1)*5):(hand*5))])
}
# Check event given that exactly two aces and two queens were dealt, that both aces are together in one hand and both queens are together in a different hand.
checkEvent5 = function(data){
condition1 = (length(unique(ceiling(which(grepl('A', data)) /5))) == 1)
condition2 = (length(unique(ceiling(which(grepl('Q', data)) /5))) == 1)
condition3 = ((ceiling(which(grepl('A', data)) /5)) != (ceiling(which(grepl('Q', data)) /5)))
return(condition1 & condition2 & condition3)
}
# Check event given that exactly two aces and two queens were dealt with the two aces together in a hand and the two queens together in a hand that all four of those cards are together in the same hand.
checkEvent6 = function(data){
return(unique(ceiling(which(grepl('A', data)) /5)) == unique(ceiling(which(grepl('Q', data)) /5)))
}
# Calculate probability that exactly 3 aces are dealt
mean(apply(simulatedData, 2, checkEvent1))
# Calculate probability that 4 hands have exactly one ace each
mean(apply(simulatedData, 2, checkEvent2))
# Calculate probability that one hand has all four aces
mean(apply(simulatedData, 2, checkEvent3))
# Calculate probability given that the ace of spades was among the cards dealt, that it is in a hand with at least one other spade.
simulatedData_reduced = simulatedData[, apply(simulatedData, 2, function(data){return('AS' %in% data)})]
mean(apply(simulatedData_reduced, 2, checkEvent4))
# Calculate probability given that exactly two aces and two queens were dealt, find the probability that both aces are together in one hand and both queens are together in a different hand.
simulatedData_reduced = simulatedData[, apply(simulatedData, 2, function(data){return('AS' %in% data)})]
mean(apply(simulatedData_reduced, 2, checkEvent4))
# Calculate probability given that exactly two aces and two queens were dealt, find the probability that both aces are together in one hand and both queens are together in a different hand.
simulatedData_reduced = simulatedData[, apply(simulatedData, 2, function(data){return('AS' %in% data)})]
mean(apply(simulatedData_reduced, 2, checkEvent4))
# Calculate probability given that exactly two aces and two queens were dealt, find the probability that both aces are together in one hand and both queens are together in a different hand.
simulatedData_reduced = simulatedData[, apply(simulatedData, 2, function(data){return((sum(grepl('A', data)) == 2) & sum(grepl('Q', data)) == 2)})]
mean(apply(simulatedData_reduced, 2, checkEvent5))
x = [1, 2]
y = [3, 3]
x = [1, 2]
x = c(1, 2)
y = c(3, 3)
x != y
y = c(4, 5)
x != y
x = c(2, 3)
y = c(2, 3)
x != y
# Calculate probability given that exactly two aces and two queens were dealt with the two aces together in a hand and the two queens together in a hand, find the probability that all four of those cards are together in the same hand.
simulatedData_reduced = simulatedData[, apply(simulatedData, 2, function(data){return((sum(grepl('A', data)) == 2) & sum(grepl('Q', data)) == 2 & length(unique(ceiling(which(grepl('A', data)) /5))) == 1 & length(unique(ceiling(which(grepl('Q', data)) /5))) == 1)})]
mean(apply(simulatedData_reduced, 2, checkEvent6))
(pbinom(2, 10, 0.1) * 0.2) / (pbinom(2, 10, 0.1) * 0.2 + pbinom(2, 10, 0.3) * 0.4 + pbinom(2, 10, 0.8) * 0.4)
(pbinom(2, 10, 0.1) * 0.3) / (pbinom(2, 10, 0.1) * 0.2 + pbinom(2, 10, 0.3) * 0.4 + pbinom(2, 10, 0.8) * 0.4)
(pbinom(2, 10, 0.1) * 0.4) / (pbinom(2, 10, 0.1) * 0.2 + pbinom(2, 10, 0.3) * 0.4 + pbinom(2, 10, 0.8) * 0.4)
(pbinom(2, 10, 0.1) * 0.4) / (pbinom(2, 10, 0.1) * 0.4 + pbinom(2, 10, 0.3) * 0.4 + pbinom(2, 10, 0.8) * 0.4)
(pbinom(2, 10, 0.1) * 0.4) / (pbinom(2, 10, 0.1) * 0.4 + pbinom(2, 10, 0.3) * 0.4 + pbinom(2, 10, 0.8) * 0.4)
(pbinom(2, 10, 0.1) * 0.4) / (pbinom(2, 10, 0.1) * 0.4 + pbinom(2, 10, 0.3) * 0.4 + pbinom(2, 10, 0.8) * 0.2)
pexp(20*5*365, 1/10000)
pexp(23*5*365, 1/10000)
pexp(16*5*365, 1/10000)
pexp(10*5*365, 1/10000)
1-pexp(20*5*365, 1/10000)
1-pexp(20*5*365, 1/20000)
pexp(20*5*365, 1/20000)
pexp(20*5*365, 1/5000)
pexp(23*5*365, 1/5000)
pexp(23*10*365, 1/5000)
pexp(20*2*365, 1/5000)
pexp(20*1*365, 1/5000)
1-pexp(20*1*365, 1/5000)
z = (80-(85/15))
z
z = (80-(85/15)) / sqrt(75/(15^2))
z
sqrt(75/(15^2))
80-(85/15)
z = (80-(85/15)) / sqrt(75/15)
z
z = (80-85) / sqrt(75/15)
z
1-pnorm(z)
1-((75/15)/((75/15)+80^2))
1-((75/15)/((75/15)+5^2))
(75/15)/((75/15)+5^2)
# Load essential libraries
library(ggplot2)
library(dplyr)
library(HSAUR)
library(ggcorrplot)
library(reshape2)
# Simulate age of 100 senior adults in years
n = 100
age_years = round(rnorm(n, mean = 50, sd = 10), 0)
print(age_years)
# Create a copy of the age variable to reflect age in months
age_months = 12*age_years
print(age_months)
# Create another copy that adds a bit of noise to the age (in years) variable
age_noisy = age_years + rnorm(n, mean = 0, sd = 4)
print(age_noisy)
# Simulate a population model for height as a function of age (in years)
height = 20 + 3*age_years + rnorm(n, mean = 0, sd = 10)
print(height)
plot(age_years, height)
plot(age_years, height)
height
age_years
plot(age_years, height)
# Create a dataframe with all the predictors and the response
hData = data.frame(age_years, age_months, age_noisy, height)
head(hData)
# Create a dataframe with all the predictors and the response
hData = data.frame(age_years, age_months, age_noisy, height)
head(hData)
model = lm(data = hData, height ~ age_years + age_months)
summary(model)
# A quick way to visualize the relationship between the response and individual predictors
pairs(hData)
# Calculate correlation matrix for all predictors
hData %>% select(-c(height))
#cormat =
# A quick way to visualize the relationship between the response and individual predictors
pairs(hData)
cormat
# Calculate correlation matrix for all predictors
cormat = cor(hData %>% select(-c(height)))
cormat
# Calculate correlation matrix for all predictors
cormat = round(cor(hData %>% select(-c(height))), 2)
cormat
# Set lower-triangular part of correlation matrix to NA
cormat[lower.tri(cormat)] = NA
print(cormat)
# Melt the correlation matrix
melt(cormat)
print(cormat)
melt(cormat)
# Visualize the correlation matrix using ggplot
p = ggplot() + geom_tile(color = 'white') +
coord_fixed() +
scale_fill_gradient2(low = 'blue', high = 'red', mid = 'white', midpoint = 0, limit = c(-1, 1), space = 'Lab', name = 'Correlation')
p
# Visualize the correlation matrix using ggplot
p = ggplot() + geom_tile(color = 'white') +
coord_fixed() +
scale_fill_gradient2(low = 'blue', high = 'red', mid = 'white', midpoint = 0, limit = c(-1, 1), space = 'Lab', name = 'Correlation')
p
p
# Visualize the correlation matrix using ggplot
p = ggplot() + geom_tile(color = 'white') +
coord_fixed() +
scale_fill_gradient2(low = 'blue', high = 'red', mid = 'white', midpoint = 0, limit = c(-1, 1), space = 'Lab', name = 'Correlation')
p
# Visualize the correlation matrix using ggplot
p = ggplot() + geom_tile(color = 'white') +
coord_fixed() +
scale_fill_gradient2(low = 'blue', high = 'red', mid = 'white', midpoint = 0, limit = c(-1, 1), space = 'Lab', name = 'Correlation')
p
# Visualize the correlation matrix using ggplot
p = ggplot() + geom_tile(color = 'white') +
coord_fixed() +
scale_fill_gradient2(low = 'blue', high = 'red', mid = 'white', midpoint = 0, limit = c(-1, 1), space = 'Lab', name = 'Correlation')
p
# Visualize the correlation matrix using ggplot
p = ggplot() + geom_tile(color = 'white') +
coord_fixed() +
scale_fill_gradient2(low = 'blue', high = 'red', mid = 'white', midpoint = 0, limit = c(-1, 1), space = 'Lab', name = 'Correlation')
p
# Visualize the correlation matrix using ggplot
p = ggplot() + geom_tile(color = 'white') +
coord_fixed() +
scale_fill_gradient2(low = 'blue', high = 'red', mid = 'white', midpoint = 0, limit = c(-1, 1), space = 'Lab', name = 'Correlation')
p
dbinom(8, 10, 0.5)
dbinom(7, 10, 0.5)
1-pbinom(6, 10, 0.5)
1-pbinom(7, 10, 0.5)
1-pbinom(8, 10, 0.5)
# Load essential libraries
library(ggplot2)
library(dplyr)
library(car)
library(glmnet)
# Load the house price dataset
hData = read.csv('../Data/houseprices_cleaned.csv', header = TRUE, stringsAsFactors = FALSE, na.strings = c("", "NA", "Not available", "Not Available"))
# Convert 'locality', 'facing' and 'parking' columns to factors
categorical_cols = c('locality', 'facing', 'parking')
hData[categorical_cols] = lapply(hData[categorical_cols], as.factor)
# Continuous columns
continuous_cols = setdiff(colnames(hData), categorical_cols)
# Percentage of NAs in each column of the data frame
hData_NA = setNames(stack(sapply(hData, function(x){(sum(is.na(x))/length(x))*100}))[2:1], c('Feature','Value'))
p = ggplot(data = hData_NA, aes(x = Feature, y = Value)) +
geom_bar(stat = 'identity', fill = 'steelblue', width = 0.3) +
theme(text = element_text(size = 14, face = 'bold'),
axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
xlab('') + ylab('Percentage') +
ggtitle('Percentage of NAs across all features')
p
# Add NA as a factor level for categorical columns
hData[categorical_cols] = lapply(hData[categorical_cols], addNA)
# Make a histogram of rent values
p = ggplot(data = hData) +
geom_histogram(aes(x = rent, y = after_stat(count)), breaks = seq(mean(hData$rent)-4*sd(hData$rent), mean(hData$rent)+4*sd(hData$rent), by = 25000), color = 'black', fill = 'blue') +
labs(x = 'Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of house rents')
# Make a histogram of rent values
p = ggplot(data = hData) +
geom_histogram(aes(x = rent, y = after_stat(count)), breaks = seq(mean(hData$rent)-4*sd(hData$rent), mean(hData$rent)+4*sd(hData$rent), by = 25000), color = 'black', fill = 'blue') +
labs(x = 'Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of house rents')
summary(model)
# Build a linear model to predict sqrt of price per sqft as a function of logarea, logrent, and parking
model = lm(data = hData, sqrtprice_per_sqft ~ logarea + logrent + parking)
setwd("D:/OneDrive - Manipal Academy of Higher Education/EvenSemester2024/AML5201/Codes/AIML")
# Percentage of NAs in each column of the data frame
hData_NA = setNames(stack(sapply(hData, function(x){(sum(is.na(x))/length(x))*100}))[2:1], c('Feature','Value'))
p = ggplot(data = hData_NA, aes(x = Feature, y = Value)) +
geom_bar(stat = 'identity', fill = 'steelblue', width = 0.3) +
theme(text = element_text(size = 14, face = 'bold'),
axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
xlab('') + ylab('Percentage') +
ggtitle('Percentage of NAs across all features')
p
# Add NA as a factor level for categorical columns
hData[categorical_cols] = lapply(hData[categorical_cols], addNA)
# Make a histogram of rent values
p = ggplot(data = hData) +
geom_histogram(aes(x = rent, y = after_stat(count)), breaks = seq(mean(hData$rent)-4*sd(hData$rent), mean(hData$rent)+4*sd(hData$rent), by = 25000), color = 'black', fill = 'blue') +
labs(x = 'Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of house rents')
# Load essential libraries
library(ggplot2)
library(dplyr)
library(car)
library(glmnet)
# Load the house price dataset
hData = read.csv('../Data/houseprices_cleaned.csv', header = TRUE, stringsAsFactors = FALSE, na.strings = c("", "NA", "Not available", "Not Available"))
str(hData)
# Convert 'locality', 'facing' and 'parking' columns to factors
categorical_cols = c('locality', 'facing', 'parking')
hData[categorical_cols] = lapply(hData[categorical_cols], as.factor)
str(hData)
# Continuous columns
continuous_cols = setdiff(colnames(hData), categorical_cols)
# Percentage of NAs in each column of the data frame
hData_NA = setNames(stack(sapply(hData, function(x){(sum(is.na(x))/length(x))*100}))[2:1], c('Feature','Value'))
p = ggplot(data = hData_NA, aes(x = Feature, y = Value)) +
geom_bar(stat = 'identity', fill = 'steelblue', width = 0.3) +
theme(text = element_text(size = 14, face = 'bold'),
axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5)) +
xlab('') + ylab('Percentage') +
ggtitle('Percentage of NAs across all features')
p
# Add NA as a factor level for categorical columns
hData[categorical_cols] = lapply(hData[categorical_cols], addNA)
# Build a linear model to predict price per square feet as a function of rent
model = lm(data = hData, price_per_sqft ~ rent)
summary(model)
# Make a histogram of transformed rent values
hData['logrent'] = log(hData['rent'])
p = ggplot(data = hData) +
geom_histogram(aes(x = logrent, y = after_stat(count)), breaks = seq(mean(hData$logrent)-4*sd(hData$logrent), mean(hData$logrent)+4*sd(hData$logrent), by = 0.5), color = 'black', fill = 'blue') +
labs(x = 'Rent', y = 'Frequency')  +
theme(axis.text = element_text(size = 8),
axis.text.x = element_text(size = 10),
axis.text.y = element_text(size = 10),
axis.title = element_text(size = 10, face = "bold"))  +
ggtitle('Histogram of house rents')
p
# Build a linear model to predict price per square feet as a function of logrent
model = lm(data = hData, price_per_sqft ~ logrent)
summary(model)
# Build a linear model to predict log of price per square feet as a function of logrent
hData['logprice_per_sqft'] = log(hData['price_per_sqft'])
model = lm(data = hData, logprice_per_sqft ~ logrent)
summary(model)
# Build a linear model to predict sqrt of price per square feet as a function of logrent
hData['sqrtprice_per_sqft'] = sqrt(hData['price_per_sqft'])
model = lm(data = hData, sqrtprice_per_sqft ~ logrent)
summary(model)
# Build a linear model to predict sqrt of price per sqft as a function of logrent and facing
model = lm(data = hData, sqrtprice_per_sqft ~ logrent + facing)
summary(model)
# Construct OLS solution obtained using theoretical formulation for predicting price per sqft as a function of continuous predictor logrent and categorical predictor facing
# Response values
y = ?
# Design matrix
X = ?
# Solve for the coefficient estimates
betahat = ?
print(betahat)
# Some properties of the residual vector
# Build model for predicting price per sqft as a function of continuous predictor logrent and categorical predictor facing
model = ?
# Sum of residuals
# Sample mean of the true response value
# Sample mean of the fitted response values
# Residuals are positively correlated with the true response values
# Residuals are uncorrelated with the predicted response values
# Build a linear model to predict sqrt of price per sqft as a function of area and logrent
model = lm(data = hData, sqrtprice_per_sqft ~ area + logrent)
summary(model)
# Build a linear model to predict sqrt of price per sqft as a function of logarea and logrent
hData['logarea'] = log(hData['area'])
model = lm(data = hData, sqrtprice_per_sqft ~ logarea + logrent)
summary(model)
# Build a linear model to predict sqrt of price per sqft as a function of logarea, logrent, and parking
model = lm(data = hData, sqrtprice_per_sqft ~ logarea + logrent + parking)
summary(model)
# Build a linear model to predict price per sqft as a function of area, rent, and parking
model = lm(data = hData, price_per_sqft ~ area + rent + parking)
summary(model)
contrast(hData$parking)
contrasts(hData$parking)
summary(model)
model = lm(data = hData, price_per_sqft ~ rent)
summary(model)
model = lm(data = hData, price_per_sqft ~ rent + area)
summary(model)
model = lm(data = hData, price_per_sqft ~ area)
summary(model)
model = lm(data = hData, price_per_sqft ~ rent + area)
summary(model)
model = lm(data = hData, price_per_sqft ~ rent + area + parking)
summary(model)
setwd("D:/OneDrive - Manipal Academy of Higher Education/EvenSemester2024/AML5201/Codes")
library(ggplot2)
library(dplyr)
# Noisy sin function as a population model
nsamples = 1000
x = seq(0, 2*pi, length = nsamples)
y = sin(x) + rnorm(length(x), mean = 0, sd = 0.1)
popData = data.frame(x, y)
colnames(popData) = c('X', 'Y')
ggplot(data = popData, aes(x = X, y = Y)) +
geom_point(size = 1, color = 'blue') +
geom_smooth(method = lm, formula = y ~ x, color = 'red', se = FALSE) +
labs(x = 'X', y = 'Y') +
ggtitle("Population Data with Population Regression Line") +
theme(axis.text = element_text(size = 12),
axis.text.x = element_text(size = 14),
axis.text.y = element_text(size = 14),
axis.title = element_text(size = 16, face = "bold"))
# Simple linear regression model and coefficient estimates
model = lm(data = popData, Y~X)
summary(model)
beta0 = summary(model)$coefficients["(Intercept)", "Estimate"]
beta1= summary(model)$coefficients["X", "Estimate"]
print(beta0)
print(beta1)
# Fit SLRM using multiple datasets
ndatasets = 1000
nsamples = 100
beta_0_hat = numeric(ndatasets)
beta_1_hat = numeric(ndatasets)
serrorbeta_0  = numeric(ndatasets)
serrorbeta_1  = numeric(ndatasets)
for (j in seq(1, ndatasets)) {
idx = sample(nrow(popData), nsamples)
model = lm(data = popData[idx,], Y~X)
beta_0_hat[j] = summary(model)$coefficients["(Intercept)", "Estimate"]
beta_1_hat[j] = summary(model)$coefficients["X", "Estimate"]
serrorbeta_0[j] = summary(model)$coefficients["(Intercept)", "Std. Error"]
serrorbeta_1[j] = summary(model)$coefficients["X", "Std. Error"]
}
# Plot histogram of estimates from different datasets
dfbetaHat = as.data.frame(cbind(beta_0_hat, beta_1_hat, serrorbeta_0, serrorbeta_1))
colnames(dfbetaHat) = c('beta0hat', 'beta1hat', 'serrorbeta0hat', 'serrorbeta1hat')
beta = beta_0_hat
mu = mean(beta)
sigma = sd(beta)
delta = 0.1 # bin width for histogram
ggplot(data = dfbetaHat) +
geom_histogram(aes(x = beta0hat, y = ..count..),
breaks = seq(mu-4*sigma, mu+4*sigma, by = delta),
colour = 'black', fill = 'steelblue', alpha = 0.4) +
labs(x = 'Beta0 Estimates', y = 'Frequency')+
ggtitle('Histogram of Intercept Estimates') +
theme(axis.text = element_text(size = 12),
axis.text.x = element_text(size = 12),
axis.text.y = element_text(size = 12),
axis.title = element_text(size = 14, face = "bold"))
beta_0_hat
dfbetaHat
beta0
mean(dfbetaHat$beta0hat)
beta1
mean(dfbetaHat$beta1hat)
setwd("D:/OneDrive - Manipal Academy of Higher Education/EvenSemester2024/AML5201/Codes/AIML")
model = lm(data = hData, price_per_sqft ~  rent + area + parking)
summary(model)
dgBetahat
dfbetaHat
# Build a linear model to predict sqrt of price per sqft as a function of logarea and logrent
hData['logarea'] = log(hData['area'])
model = lm(data = hData, sqrtprice_per_sqft ~ logarea + logrent)
summary(model)
dbinom(8, n = 10, p = 0.5)
dbinom(8, 10, 0.5)
dbinom(9, 10, 0.5)
1-pbinom(7, 10, 0.5)
dbinom(8, 10, 0.5) + dbinom(9, 10, 0.5) + dbinom(10, 10, 0.5)
model = lm(data = hData, price_per_sqft ~ rent + area)
model = lm(data = hData, price_per_sqft ~ rent + area)
summary(model)
model = lm(data = hData, price_per_sqft ~ rent + area + parking)
summary(model)
model = lm(data = hData, price_per_sqft ~ rent + facing)
summary(model)
